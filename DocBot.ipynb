{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d9102dc24cca4fcb9b97f05445d49040": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8713a8a4699c4e36a64c9c93c4e2cb98",
              "IPY_MODEL_99215d6f91e843fc80148daed3ca4df0",
              "IPY_MODEL_bcbc8dc71cdd49b28195f5f44168c778"
            ],
            "layout": "IPY_MODEL_c2cc342da0f04f9ea4c9a0c2849f6681"
          }
        },
        "8713a8a4699c4e36a64c9c93c4e2cb98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab1e775ef8cd45029ed2d5fcf19d9ee9",
            "placeholder": "​",
            "style": "IPY_MODEL_e030b14446e74017aa38b822334e7604",
            "value": "Map: 100%"
          }
        },
        "99215d6f91e843fc80148daed3ca4df0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8315d1be5ed34d9b9ffbc75170679667",
            "max": 5049,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ab7ac3850db46df8217b263d4cc993f",
            "value": 5049
          }
        },
        "bcbc8dc71cdd49b28195f5f44168c778": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7eaa192bd97478e9a6d864b38cda0ac",
            "placeholder": "​",
            "style": "IPY_MODEL_a3b6b4b62f58452b8c825d22bdef3462",
            "value": " 5049/5049 [01:05&lt;00:00, 90.83 examples/s]"
          }
        },
        "c2cc342da0f04f9ea4c9a0c2849f6681": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab1e775ef8cd45029ed2d5fcf19d9ee9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e030b14446e74017aa38b822334e7604": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8315d1be5ed34d9b9ffbc75170679667": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ab7ac3850db46df8217b263d4cc993f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b7eaa192bd97478e9a6d864b38cda0ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3b6b4b62f58452b8c825d22bdef3462": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bpf9eDhy5wDY",
        "outputId": "361c9923-384c-4e81-eec5-05da0d5383b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Install the necessary libraries\n",
        "!pip install transformers datasets nltk\n",
        "\n",
        "# Mount Google Drive to access the BioASQ dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import json\n",
        "import os\n",
        "from transformers import BertTokenizer, BertForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from datasets import Dataset\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Define the path to BioASQ data in Google Drive\n",
        "bioasq_path = '/content/drive/My Drive/Fall2024/CS410/FinalProject/training12b_new.json'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BioASQProcessor:\n",
        "    def __init__(self, bioasq_path):\n",
        "        self.punctuations = '\"\\\\,<>./?@#$%^&*_~/!()-[]{};:\\''\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.bioasq_path = bioasq_path\n",
        "        self.dataset = self.load_dataset()\n",
        "        self.info = self.collect_info()\n",
        "\n",
        "    def load_dataset(self):\n",
        "        # Load BioASQ dataset\n",
        "        with open(self.bioasq_path, 'r') as file:\n",
        "            return json.load(file)\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        # Lowercase and remove HTML\n",
        "        text = text.lower().strip()\n",
        "        if '<' in text:\n",
        "            text = text.split('<')[0]\n",
        "        if '>' in text:\n",
        "            text = text.split('>')[-1]\n",
        "\n",
        "        # Remove punctuation, tokenize, remove stopwords, and lemmatize\n",
        "        clean_text = ''.join(char for char in text if char not in self.punctuations and not char.isdigit())\n",
        "        words = clean_text.split()\n",
        "        clean_words = [self.lemmatizer.lemmatize(word) for word in words if word not in self.stop_words]\n",
        "\n",
        "        return ' '.join(clean_words)\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        # Preprocess questions, snippets, and answers in each entry\n",
        "        for entry in self.dataset['questions']:\n",
        "            entry['body'] = self.preprocess_text(entry['body'])\n",
        "            for snippet in entry['snippets']:\n",
        "                snippet['text'] = self.preprocess_text(snippet['text'])\n",
        "            if 'ideal_answer' in entry:\n",
        "                entry['ideal_answer'] = [self.preprocess_text(ans) for ans in entry['ideal_answer']]\n",
        "            if 'concepts' in entry:\n",
        "                entry['concepts'] = [self.preprocess_text(con) for con in entry['concepts']]\n",
        "        return self.dataset\n",
        "\n",
        "    def collect_info(self):\n",
        "        # Extract and display information about the dataset\n",
        "        num_entries = len(self.dataset['questions'])\n",
        "        dataset_size = os.path.getsize(self.bioasq_path) / 1024  # Size in KB\n",
        "        vocab = set()\n",
        "\n",
        "        for entry in self.dataset['questions']:\n",
        "            vocab.update(entry['body'].split())\n",
        "            for snippet in entry['snippets']:\n",
        "                vocab.update(snippet['text'].split())\n",
        "            if 'ideal_answer' in entry:\n",
        "                for ans in entry['ideal_answer']:\n",
        "                    vocab.update(ans.split())\n",
        "            if 'concepts' in entry:\n",
        "                vocab.update(entry['concepts'])\n",
        "\n",
        "        return {\n",
        "            'num_entries': num_entries,\n",
        "            'dataset_size_kb': dataset_size,\n",
        "            'vocab_size': len(vocab),\n",
        "            'avg_words_per_entry': sum(len(entry['body'].split()) + sum(len(snippet['text'].split()) for snippet in entry['snippets']) for entry in self.dataset['questions']) / num_entries,\n",
        "        }\n",
        "\n",
        "# Process BioASQ Data\n",
        "bioasq_processor = BioASQProcessor(bioasq_path)\n",
        "preprocessed_data = bioasq_processor.preprocess_data()\n",
        "info = bioasq_processor.info\n",
        "print(\"Dataset Info:\", info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtzG4HwD6foW",
        "outputId": "d4504dd3-1c71-4a67-b883-5a64c7dc52b8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Info: {'num_entries': 5049, 'dataset_size_kb': 38994.5361328125, 'vocab_size': 138371, 'avg_words_per_entry': 348.4848484848485}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BioASQProcessor:\n",
        "    def __init__(self, bioasq_path):\n",
        "        self.punctuations = '\"\\\\,<>./?@#$%^&*_~/!()-[]{};:\\''\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.bioasq_path = bioasq_path\n",
        "        self.dataset = self.load_dataset()\n",
        "        self.info = self.collect_info()\n",
        "\n",
        "    def load_dataset(self):\n",
        "        # Load BioASQ dataset\n",
        "        with open(self.bioasq_path, 'r') as file:\n",
        "            return json.load(file)\n",
        "\n",
        "    def preprocess_text(self, text, word_counter):\n",
        "        # Count words before preprocessing\n",
        "        word_counter['before'] += len(text.split())\n",
        "\n",
        "        # Lowercase and remove HTML\n",
        "        text = text.lower().strip()\n",
        "        if '<' in text:\n",
        "            text = text.split('<')[0]\n",
        "        if '>' in text:\n",
        "            text = text.split('>')[-1]\n",
        "\n",
        "        # Remove punctuation, tokenize, remove stopwords, and lemmatize\n",
        "        clean_text = ''.join(char for char in text if char not in self.punctuations and not char.isdigit())\n",
        "        words = clean_text.split()\n",
        "        clean_words = [self.lemmatizer.lemmatize(word) for word in words if word not in self.stop_words]\n",
        "\n",
        "        # Count words after preprocessing\n",
        "        word_counter['after'] += len(clean_words)\n",
        "        word_counter['unique_words'].update(clean_words)\n",
        "\n",
        "        return ' '.join(clean_words)\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        word_counter = {'before': 0, 'after': 0, 'unique_words': set()}\n",
        "\n",
        "        # Preprocess questions, snippets, and answers in each entry\n",
        "        for entry in self.dataset['questions']:\n",
        "            entry['body'] = self.preprocess_text(entry['body'], word_counter)\n",
        "            for snippet in entry['snippets']:\n",
        "                snippet['text'] = self.preprocess_text(snippet['text'], word_counter)\n",
        "            if 'ideal_answer' in entry:\n",
        "                entry['ideal_answer'] = [self.preprocess_text(ans, word_counter) for ans in entry['ideal_answer']]\n",
        "            if 'concepts' in entry:\n",
        "                entry['concepts'] = [self.preprocess_text(con, word_counter) for con in entry['concepts']]\n",
        "\n",
        "        # Add word count info to the dataset info\n",
        "        self.info['word_count_before'] = word_counter['before']\n",
        "        self.info['word_count_after'] = word_counter['after']\n",
        "        self.info['unique_words_after'] = len(word_counter['unique_words'])\n",
        "\n",
        "        return self.dataset\n",
        "\n",
        "    def collect_info(self):\n",
        "        # Extract and display information about the dataset\n",
        "        num_entries = len(self.dataset['questions'])\n",
        "        dataset_size = os.path.getsize(self.bioasq_path) / 1024  # Size in KB\n",
        "        vocab = set()\n",
        "\n",
        "        for entry in self.dataset['questions']:\n",
        "            vocab.update(entry['body'].split())\n",
        "            for snippet in entry['snippets']:\n",
        "                vocab.update(snippet['text'].split())\n",
        "            if 'ideal_answer' in entry:\n",
        "                for ans in entry['ideal_answer']:\n",
        "                    vocab.update(ans.split())\n",
        "            if 'concepts' in entry:\n",
        "                vocab.update(entry['concepts'])\n",
        "\n",
        "        return {\n",
        "            'num_entries': num_entries,\n",
        "            'dataset_size_kb': dataset_size,\n",
        "            'vocab_size': len(vocab),\n",
        "            'avg_words_per_entry': sum(len(entry['body'].split()) + sum(len(snippet['text'].split()) for snippet in entry['snippets']) for entry in self.dataset['questions']) / num_entries,\n",
        "        }\n",
        "\n",
        "# Process BioASQ Data\n",
        "bioasq_processor = BioASQProcessor(bioasq_path)\n",
        "preprocessed_data = bioasq_processor.preprocess_data()\n",
        "info = bioasq_processor.info\n",
        "\n",
        "# Print dataset information including word counts before and after preprocessing\n",
        "print(\"Dataset Info:\", info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SbpV_Wx7Yis",
        "outputId": "9cbe8042-8a14-486c-edea-7b3e9c6795fd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Info: {'num_entries': 5049, 'dataset_size_kb': 38994.5361328125, 'vocab_size': 138371, 'avg_words_per_entry': 348.4848484848485, 'word_count_before': 2104161, 'word_count_after': 1345637, 'unique_words_after': 57154}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "print(json.dumps(info, indent=4))\n"
      ],
      "metadata": {
        "id": "AxuCIrl67mI6",
        "outputId": "eee7a9be-407a-462c-bb57-f6ee0b2cbf98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"num_entries\": 5049,\n",
            "    \"dataset_size_kb\": 38994.5361328125,\n",
            "    \"vocab_size\": 138371,\n",
            "    \"avg_words_per_entry\": 348.4848484848485,\n",
            "    \"word_count_before\": 2104161,\n",
            "    \"word_count_after\": 1345637,\n",
            "    \"unique_words_after\": 57154\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from datasets import Dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbIAyI3COBsu",
        "outputId": "ac5ff814-68a1-4e7b-96c2-106776b8ca97"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer.save_pretrained(\"bioasq_tokenizer\")\n",
        "\n",
        "# Convert processed BioASQ data to text for MLM\n",
        "texts = []\n",
        "for entry in preprocessed_data['questions']:\n",
        "    question = entry['body']\n",
        "    snippets = \" \".join(snippet['text'] for snippet in entry.get('snippets', []))\n",
        "    combined_text = question + \" \" + snippets\n",
        "    texts.append(combined_text)\n",
        "\n",
        "# Create a Hugging Face Dataset and tokenize\n",
        "bioasq_dataset = Dataset.from_dict({'text': texts})\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokenized_datasets = bioasq_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "d9102dc24cca4fcb9b97f05445d49040",
            "8713a8a4699c4e36a64c9c93c4e2cb98",
            "99215d6f91e843fc80148daed3ca4df0",
            "bcbc8dc71cdd49b28195f5f44168c778",
            "c2cc342da0f04f9ea4c9a0c2849f6681",
            "ab1e775ef8cd45029ed2d5fcf19d9ee9",
            "e030b14446e74017aa38b822334e7604",
            "8315d1be5ed34d9b9ffbc75170679667",
            "1ab7ac3850db46df8217b263d4cc993f",
            "b7eaa192bd97478e9a6d864b38cda0ac",
            "a3b6b4b62f58452b8c825d22bdef3462"
          ]
        },
        "id": "0beeMW6CMglm",
        "outputId": "42e5010a-e5fe-42de-9eb8-4c24b7d28e1e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5049 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9102dc24cca4fcb9b97f05445d49040"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BERT model for MLM\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Data collator with MLM functionality\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EC33c_eyMpiR",
        "outputId": "3f707dee-e549-4132-d31e-3e5112b902e9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Define TrainingArguments with evaluation disabled\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    evaluation_strategy=\"no\",  # Disable evaluation\n",
        ")\n",
        "\n",
        "# Initialize Trainer with only train dataset\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets,  # Your training data\n",
        "    eval_dataset=None,  # Explicitly set to None to skip evaluation\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "PDox4OOEMrzD",
        "outputId": "f8332feb-a356-47f1-8572-80356af575ae"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1896' max='1896' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1896/1896 08:27, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.845500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.399900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>2.723600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1896, training_loss=2.1246726925363015, metrics={'train_runtime': 507.6458, 'train_samples_per_second': 29.838, 'train_steps_per_second': 3.735, 'total_flos': 996690824870400.0, 'train_loss': 2.1246726925363015, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./biomedical-bert\")\n",
        "tokenizer.save_pretrained(\"./biomedical-bert\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e8Y_YBXMtzx",
        "outputId": "02090f5d-a57a-4b75-9a86-5aa29ae46323"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./biomedical-bert/tokenizer_config.json',\n",
              " './biomedical-bert/special_tokens_map.json',\n",
              " './biomedical-bert/vocab.txt',\n",
              " './biomedical-bert/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "model_path = \"./biomedical-bert\"\n",
        "tokenizer_path = \"./biomedical-bert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
        "\n",
        "def get_manual_answer(question, context):\n",
        "    # Tokenize and encode question and context\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        answer_start = torch.argmax(outputs.start_logits)\n",
        "        answer_end = torch.argmax(outputs.end_logits) + 1\n",
        "        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs.input_ids[0][answer_start:answer_end]))\n",
        "    return answer\n",
        "\n",
        "def manual_testing():\n",
        "    print(\"Manual Testing of BioASQ QA Model\")\n",
        "    print(\"Enter 'quit' to exit.\")\n",
        "    while True:\n",
        "        # Input question\n",
        "        question = input(\"Enter your biomedical question: \")\n",
        "        if question.lower() == 'quit':\n",
        "            print(\"Exiting manual testing.\")\n",
        "            break\n",
        "\n",
        "        # Input optional context (or leave it blank)\n",
        "        context = input(\"Enter context (or press Enter to skip): \")\n",
        "\n",
        "        # Get answer from the model\n",
        "        answer = get_manual_answer(question, context)\n",
        "        print(f\"Answer: {answer}\\n\")\n",
        "\n",
        "# Start manual testing\n",
        "manual_testing()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GL2sjNNJWot_",
        "outputId": "9d997f7e-e1a0-4126-a08e-c63437f39d2f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ./biomedical-bert and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual Testing of BioASQ QA Model\n",
            "Enter 'quit' to exit.\n",
            "Enter your biomedical question: quit\n",
            "Exiting manual testing.\n"
          ]
        }
      ]
    }
  ]
}